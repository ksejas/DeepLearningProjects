{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlF-cPajSklV"
   },
   "source": [
    "# ORIGINAL MODEL -GPT text generation from scratch with KerasNLP\n",
    "\n",
    "**Author:** [Jesse Chan](https://github.com/jessechancy)<br>\n",
    "**Date created:** 2022/07/25<br>\n",
    "**Last modified:** 2022/07/25<br>\n",
    "**Description:** Using KerasNLP to train a mini-GPT model for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1223WqeSklX"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will use KerasNLP to build a scaled down Generative\n",
    "Pre-Trained (GPT) model. GPT is a Transformer-based model that allows you to generate\n",
    "sophisticated text from a prompt.\n",
    "\n",
    "We will train the model on the [simplebooks-92](https://arxiv.org/abs/1911.12391) corpus,\n",
    "which is a dataset made from several novels. It is a good dataset for this example since\n",
    "it has a small vocabulary and high word frequency, which is beneficial when training a\n",
    "model with few parameters.\n",
    "\n",
    "This example combines concepts from\n",
    "[Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
    "with KerasNLP abstractions. We will demonstrate how KerasNLP tokenization, layers and\n",
    "metrics simplify the training\n",
    "process, and then show how to generate output text using the KerasNLP sampling utilities.\n",
    "\n",
    "Note: If you are running this example on a Colab,\n",
    "make sure to enable GPU runtime for faster training.\n",
    "\n",
    "This example requires KerasNLP. You can install it via the following command:\n",
    "`pip install keras-nlp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3Jql5ctSklX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LA7r5D04TDjE"
   },
   "outputs": [],
   "source": [
    "#!pip install keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXKjb68J5pXh",
    "outputId": "7541416f-4b3e-4895-9101-b1cc29dfacf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/keras-team/keras-nlp.git\n",
      "  Cloning https://github.com/keras-team/keras-nlp.git to /tmp/pip-req-build-wzjbmn37\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/keras-team/keras-nlp.git /tmp/pip-req-build-wzjbmn37\n",
      "  Resolved https://github.com/keras-team/keras-nlp.git to commit 9286561f35d4727a373e135217279761edadb486\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting keras-core (from keras-nlp==0.7.0)\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.7.0) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.7.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.7.0) (23.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.7.0) (2023.6.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.7.0) (13.6.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.7.0) (0.1.8)\n",
      "Collecting tensorflow-text (from keras-nlp==0.7.0)\n",
      "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting namex (from keras-core->keras-nlp==0.7.0)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-nlp==0.7.0) (3.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp==0.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp==0.7.0) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp==0.7.0) (0.15.0)\n",
      "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp==0.7.0) (2.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp==0.7.0) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text->keras-nlp==0.7.0) (3.2.2)\n",
      "Building wheels for collected packages: keras-nlp\n",
      "  Building wheel for keras-nlp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-nlp: filename=keras_nlp-0.7.0-py3-none-any.whl size=622021 sha256=6d718dd4eda992f944fe87b2a780e15d5271f3cb45c5b804cb4b5788b3a27c9f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-70abdr81/wheels/aa/8c/45/112235850203b00e1d4942afbaa83677a6d8a775618e72a132\n",
      "Successfully built keras-nlp\n",
      "Installing collected packages: namex, keras-core, tensorflow-text, keras-nlp\n",
      "Successfully installed keras-core-0.1.7 keras-nlp-0.7.0 namex-0.0.7 tensorflow-text-2.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/keras-team/keras-nlp.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kzMXjw6SklX",
    "outputId": "c9a3cbe2-7478-4708-871f-f149fec2bc3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF6Ox8c-SklY"
   },
   "source": [
    "## Settings & hyperparameters - I decreased the batch size to 32 instead of using the example's 64 value since I kept running out of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zSeR6MggSklY"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "#BATCH_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000  # Limits parameters in model.\n",
    "\n",
    "# Training\n",
    "EPOCHS = 6\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "subOetVUSklY"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Now, let's download the dataset! The SimpleBooks dataset consists of 1,573 Gutenberg books, and has\n",
    "one of the smallest vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k,\n",
    "a third of WikiText-103's, with around the same number of tokens (~100M). This makes it easy to fit a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XX7vCF9NSklY",
    "outputId": "7c4c0fc7-f8f2-41ed-e33f-dec20e66d023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n",
      "282386239/282386239 [==============================] - 25s 0us/step\n"
     ]
    }
   ],
   "source": [
    "keras.utils.get_file(\n",
    "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# Load simplebooks-92 train set and filter out short lines.\n",
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load simplebooks-92 validation set and filter out short lines.\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wj8rX8eqRzXb",
    "outputId": "8b1991f7-df89-4806-e945-ede308fb2129"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ShuffleDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzo6I683SklZ"
   },
   "source": [
    "## Train the tokenizer\n",
    "\n",
    "We train the tokenizer from the training dataset for a vocabulary size of `VOCAB_SIZE`,\n",
    "which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, as\n",
    "we will see later on\n",
    "that it has a large effect on the number of model parameters. We also don't want to include\n",
    "*too few* vocabulary terms, or there would be too many out-of-vocabulary (OOV) sub-words. In\n",
    "addition, three tokens are reserved in the vocabulary:\n",
    "\n",
    "- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both\n",
    "`reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider\n",
    "`0`/`vocab[0]` as the default padding.\n",
    "- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n",
    "`WordPieceTokenizer`.\n",
    "- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token\n",
    "representing the beginning of each line of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rtx49KHfSklZ"
   },
   "outputs": [],
   "source": [
    "# Train tokenizer vocabulary\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxOgpIFUSklZ"
   },
   "source": [
    "## Load tokenizer\n",
    "\n",
    "We use the vocabulary data to initialize\n",
    "`keras_nlp.tokenizers.WordPieceTokenizer`. WordPieceTokenizer is an efficient\n",
    "implementation of the WordPiece algorithm used by BERT and other models. It will strip,\n",
    "lower-case and do other irreversible preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7c2_PgEKSklZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN_zyJqHSklZ"
   },
   "source": [
    "## Tokenize data\n",
    "\n",
    "We preprocess the dataset by tokenizing and splitting it into `features` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WZIiev6iSkla"
   },
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Tokenize and split into train and label sequences.\n",
    "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYwnsp3_Skla"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "We create our scaled down GPT model with the following layers:\n",
    "\n",
    "- One `keras_nlp.layers.TokenAndPositionEmbedding` layer, which combines the embedding\n",
    "for the token and its position.\n",
    "- Multiple `keras_nlp.layers.TransformerDecoder` layers, with the default causal masking.\n",
    "The layer has no cross-attention when run with decoder sequence only.\n",
    "- One final dense linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8PmdwsOSkla"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "# Embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoders.\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "# Output.\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEwMTb_fSkla"
   },
   "source": [
    "Let's take a look at our model summary - a large majority of the\n",
    "parameters are in the `token_and_position_embedding` and the output `dense` layer!\n",
    "This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n",
    "while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1kUdf5kSkla",
    "outputId": "803e1622-5a26-4ecc-d31b-7023ccc02189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_decoder (Trans  (None, None, 256)         394749    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3387266 (12.92 MB)\n",
      "Trainable params: 3387266 (12.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0Mbkg8Skla"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NYNZaYDSkla",
    "outputId": "23d911ef-4432-4697-d334-354ad03e7073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3169/3169 - 297s - loss: 4.4864 - perplexity: 89.1600 - val_loss: 4.0919 - val_perplexity: 60.5466 - 297s/epoch - 94ms/step\n",
      "Epoch 2/6\n",
      "3169/3169 - 233s - loss: 4.0451 - perplexity: 57.3399 - val_loss: 3.9982 - val_perplexity: 55.0629 - 233s/epoch - 74ms/step\n",
      "Epoch 3/6\n",
      "3169/3169 - 238s - loss: 3.9384 - perplexity: 51.5314 - val_loss: 3.9162 - val_perplexity: 50.6466 - 238s/epoch - 75ms/step\n",
      "Epoch 4/6\n",
      "3169/3169 - 235s - loss: 3.8803 - perplexity: 48.6230 - val_loss: 3.8909 - val_perplexity: 49.5133 - 235s/epoch - 74ms/step\n",
      "Epoch 5/6\n",
      "3169/3169 - 235s - loss: 3.8391 - perplexity: 46.6601 - val_loss: 3.8705 - val_perplexity: 48.3285 - 235s/epoch - 74ms/step\n",
      "Epoch 6/6\n",
      "3169/3169 - 234s - loss: 3.8097 - perplexity: 45.3073 - val_loss: 3.8503 - val_perplexity: 47.4212 - 234s/epoch - 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6e813cfd30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-Vt2IkOSkla"
   },
   "source": [
    "## Inference\n",
    "\n",
    "With our trained model, we can test it out to gauge its performance. To do this\n",
    "we can seed our model with an input sequence starting with the `\"[BOS]\"` token,\n",
    "and progressively sample the model by making predictions for each subsequent\n",
    "token in a loop.\n",
    "\n",
    "To start lets build a prompt with the same shape as our model inputs, containing\n",
    "only the `\"[BOS]\"` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJ7IJwqUSkla",
    "outputId": "9ca33297-f684-4a4b-edd8-55bf039acb8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"packer\" layers adds the [BOS] token for us.\n",
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbttSuQISkla"
   },
   "source": [
    "We will use the `keras_nlp.samplers` module for inference, which requires a\n",
    "callback function wrapping the model we just trained. This wrapper calls\n",
    "the model and returns the logit predictions for the current token we are\n",
    "generating.\n",
    "\n",
    "Note: There are two pieces of more advanced functionality available when\n",
    "defining your callback. The first is the ability to take in a `cache` of states\n",
    "computed in previous generation steps, which can be used to speed up generation.\n",
    "The second is the ability to output the final dense \"hidden state\" of each\n",
    "generated token. This is used by `keras_nlp.samplers.ContrastiveSampler`, which\n",
    "avoids repetition by penalizing repeated hidden states. Both are optional, and\n",
    "we will ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ6cH801Skla"
   },
   "outputs": [],
   "source": [
    "\n",
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh8hb8I8Skla"
   },
   "source": [
    "Creating the wrapper function is the most complex part of using these functions. Now that\n",
    "it's done, let's test out the different utilities, starting with greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRWVnR0Skla"
   },
   "source": [
    "### Greedy search\n",
    "\n",
    "We greedily pick the most probable token at each timestep. In other words, we get the\n",
    "argmax of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4lwlc2hSkla",
    "outputId": "64ee7c0f-da2b-41e9-cd2a-ec9803b25225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "[b'[BOS] \" i have not heard that the king \\' s son was a king , and that he was a king , and that he was king of the king of the king , and that he was king of the king of the king of the confederacy , and that he was king of the king of the king of the king of the confederacy , and that he was king of the king of the king , and that he was so great that he was that he was king of the king of the king of the combion that he was king of the king of the king of the c']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5PROe0kSkla"
   },
   "source": [
    "As you can see, greedy search starts out making some sense, but quickly starts repeating\n",
    "itself. This is a common problem with text generation that can be fixed by some of the\n",
    "probabilistic text generation utilities shown later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXefnfZeSkla"
   },
   "source": [
    "### Beam search\n",
    "\n",
    "At a high-level, beam search keeps track of the `num_beams` most probable sequences at\n",
    "each timestep, and predicts the best next token from all sequences. It is an improvement\n",
    "over greedy search since it stores more possibilities. However, it is less efficient than\n",
    "greedy search since it has to compute and store multiple potential sequences.\n",
    "\n",
    "**Note:** beam search with `num_beams=1` is identical to greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8rp36VRSklb",
    "outputId": "10c1141c-d93d-4055-840e-522fc29080d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generated text: \n",
      "[b'[BOS] \" i \\' ll tell you what i \\' ll tell you what i \\' ll tell you what i \\' ll tell you . i \\' ll tell you what i \\' ll tell you what i \\' ll tell you . i \\' ll tell you what i \\' ll tell you what i \\' ll tell you . i \\' ll tell you what i \\' ll tell you what i \\' ll tell you . i \\' ll tell you what i \\' ll tell you about it . i \\' ll tell you what i \\' ll tell you about it . i \\' ll tell you what i \\' ll tell you about it . i \\' ll tell you how']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuAowXN3Sklb"
   },
   "source": [
    "Similar to greedy search, beam search quickly starts repeating itself, since it is still\n",
    "a deterministic method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEco3riVSklb"
   },
   "source": [
    "### Random search\n",
    "\n",
    "Random search is our first probabilistic method. At each time step, it samples the next\n",
    "token using the softmax probabilities provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uvbZKbcSklb",
    "outputId": "483c7522-1255-44f3-f506-1c7556f3986e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search generated text: \n",
      "[b'[BOS] this was a pretty little space behind the drawing - room . one of the room had been numbered very rough and neat and timothy - looking buildings in the wiggins of the court . this building and keeping watch till close to the lighted room polly , and impassion - - things surely had become st your guest . transfers or stoop for thin mr . damhill overdillantine , in which other carrier , subject which aunt pee - weodrig order , not only i did that talking in going coats , as if one should the girl had under']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbi9Me4rSklb"
   },
   "source": [
    "Voilà, no repetitions! However, with random search, we may see some nonsensical words\n",
    "appearing since any word in the vocabulary has a chance of appearing with this sampling\n",
    "method. This is fixed by our next search utility, top-k search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DZION3TSklb"
   },
   "source": [
    "### Top-K search\n",
    "\n",
    "Similar to random search, we sample the next token from the probability distribution\n",
    "provided by the model. The only difference is that here, we select out the top `k` most\n",
    "probable tokens, and distribute the probability mass over them before sampling. This way,\n",
    "we won't be sampling from low probability tokens, and hence we would have less\n",
    "nonsensical words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGPLU2NkSklb",
    "outputId": "64a02ed9-aca5-465a-f9e7-bf316ebaa34f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "[b\"[BOS] it is not a man to be afraid of a shriekment . it must have been an awful night . it must not be cold to see it . it ' s a very bad dream and he never have seen it at all , but it ' s not a good thing . he ' s going to have a good chance , that he ' ll tells what he ' s , if he could not tell how it was that he ' s got a good idea of the coil . it ' s so he ' ll make it a nice thing to do ; and i ' m glad that he is a man of his\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEqfvLyWSklb"
   },
   "source": [
    "### Top-P search\n",
    "\n",
    "Even with the top-k search, there is something to improve upon. With top-k search, the\n",
    "number `k` is fixed, which means it selects the same number of tokens for any probability\n",
    "distribution. Consider two scenarios, one where the probability mass is concentrated over\n",
    "2 words and another where the probability mass is evenly concentrated across 10. Should\n",
    "we choose `k=2` or `k=10`? There is no one size that fits all `k` here.\n",
    "\n",
    "This is where top-p search comes in! Instead of choosing a `k`, we choose a probability\n",
    "`p` that we want the probabilities of the top tokens to sum up to. This way, we can\n",
    "dynamically adjust the `k` based on the probability distribution. By setting `p=0.9`, if\n",
    "90% of the probability mass is concentrated on the top 2 tokens, we can filter out the\n",
    "top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will\n",
    "similarly filter out the top 10 tokens to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KS2ajmgSklb",
    "outputId": "c27f78c2-7a69-48d3-d91a-304f9870475d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P search generated text: \n",
      "[b'[BOS] \" it is a very good thing to be able to do , \" said the king , and all the other continentality . \" the gruffs of the protestants have not allowed them to stay in the city . it is well that they will be well armed , and they will do it to make the concerts of the thoroughmis of the raisinster and the tubes of the navigation and the whole of the tube with the same complaints . the king has promised to pay his apac']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Q6Ok2ZSklb"
   },
   "source": [
    "### Using callbacks for text generation\n",
    "\n",
    "We can also wrap the utilities in a callback, which allows you to print out a prediction\n",
    "sequence for every epoch of the model! Here is an example of a callback for top-k search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1xnaPE9Skle",
    "outputId": "244df7cf-874e-4870-f582-93272f6e3634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Top-K search generated text: \n",
      "[b'[BOS] \" that \\' s the use , i know i \\' e \\' ve heard , \\' i \\' ve \\' em in . you see , it is , an \\' a \\' tr i \\' m going to do this \\' yer \\' n \\' a \\' i ain \\' m goin \\' ter git \\' \\' \\' to s a \\' s \\' s a \\' \\' ain \\' \\' t i \\' \\' n \\' t \\' a \\' th \\' \\' s \\' i \\' \\' em \\' s a \\' \\' \\' svin \\' \\' \\' \\' \\' t a \\' \\' \\' p \\' o \\' n \\' an \\' \\' \\' \\' \\' \\' all i']\n",
      "\n",
      "1/1 - 11s - loss: 3.8869 - perplexity: 48.9554 - 11s/epoch - 11s/step\n",
      "Epoch 2/2\n",
      "Top-K search generated text: \n",
      "[b\"[BOS] the young man was an unattaine , a compassion of the same sort of a thing ; and he was very fond of it , so he went up to his father ' s home , and went to the stable with two little kinsmores , and went to the barn to a bale , and told her to stay where he stayed and waited . when he found that they were all the way down the lane where the old woman lived , he was not at all alone in the village , but he was a man , who was only a boy ; and , too , he knew what had\"]\n",
      "\n",
      "1/1 - 11s - loss: 3.9044 - perplexity: 49.8365 - 11s/epoch - 11s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6e813c9240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "\n",
    "\n",
    "text_generation_callback = TopKTextGenerator(k=10)\n",
    "# Dummy training loop to demonstrate callback.\n",
    "model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrlv6aWHWaQ1"
   },
   "source": [
    "# Model 1 - Identical to GPT Scratch Model from the \"GPT text generation from scratch with KerasNLP\" (original model) example except with the addition of a Normalization Layer\n",
    "\n",
    "I added a normalization layer with the layer_norm_epsilon equal to 1e-5 to the original model since normalization layers have been shown to often help speed up and stabilize the learning process. A normalization layer helps prevent vanishing or exploding gradients which slows down learning. A normalization layer accomplishes this by keeping the weights and activations within a reasonable range. Additionally, I added this layer since it helps improve generalization of the model by reducing overfitting. Many of the top performing LLM's include it in their architecture.\n",
    "\n",
    "Sources:\n",
    "\n",
    "*   https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6\n",
    "*   https://huggingface.co/blog/optimize-llm\n",
    "*   https://medium.com/@minh.hoque/demystifying-neural-network-normalization-techniques-4a21d35b14f8\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDPI-cSgWXk6"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZyeM3-bWXlC"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "# Embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoders.\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "# Modified Model to include a Normalization Layer\n",
    "layer_norm_epsilon=1e-5\n",
    "sequence_output = keras.layers.LayerNormalization(\n",
    "    name=\"layer_norm\",\n",
    "    axis=-1,\n",
    "    epsilon=layer_norm_epsilon,\n",
    "    dtype=\"float32\",\n",
    ")(x)\n",
    "\n",
    "# Output.\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbeDxANlWXlC",
    "outputId": "e3cf402b-f8bb-48d8-b141-ede1b7b661d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_1 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_2 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_3 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3387266 (12.92 MB)\n",
      "Trainable params: 3387266 (12.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3aj9hbMWXlD"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8REQq5WWXlD",
    "outputId": "ddd37e02-3795-4db0-d977-1ee6b3ad75ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6337/6337 - 341s - loss: 4.3653 - perplexity: 78.9911 - val_loss: 4.0722 - val_perplexity: 59.0935 - 341s/epoch - 54ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 281s - loss: 3.9851 - perplexity: 53.9963 - val_loss: 3.9452 - val_perplexity: 52.0450 - 281s/epoch - 44ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 288s - loss: 3.8896 - perplexity: 49.0790 - val_loss: 3.8868 - val_perplexity: 49.1475 - 288s/epoch - 46ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 278s - loss: 3.8348 - perplexity: 46.4603 - val_loss: 3.8445 - val_perplexity: 47.1634 - 278s/epoch - 44ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 265s - loss: 3.7958 - perplexity: 44.6805 - val_loss: 3.8208 - val_perplexity: 46.1472 - 265s/epoch - 42ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 264s - loss: 3.7663 - perplexity: 43.3817 - val_loss: 3.7881 - val_perplexity: 44.5675 - 264s/epoch - 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7c8001370d60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soMNu8U9jswB"
   },
   "source": [
    "# Model 2 - Identical to GPT Scratch Model from the \"GPT text generation from scratch with KerasNLP\" (original model) example except with the addition of a Normalization Layer. This model is identical to Model 1 except in regards to the placement of the normalization layer. In Model 1 the normalization layer was added after the transformer decoders while in Model 2 it is added as part of the transformer decoders (keras_nlp.layers.TransformerDecoder), specifically I use the normalize_first argument and set it to true so the inputs to the attention layer and the intermediate dense layer are normalized in a fashion similar to a GPT-2 model. For both Model 1 and 2, I utilize 1e-5 as the eps value in the layer normalization component.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfgEwQZ8jswC"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTI6kOHdjswC"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "# Embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoders.\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        normalize_first=True,   #normalized similarly to gpt-2\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "# Output.\n",
    "outputs_2 = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model_2 = keras.Model(inputs=inputs, outputs=outputs_2)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model_2.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGXZKZQrjswC"
   },
   "source": [
    "Let's take a look at our model summary - a large majority of the\n",
    "parameters are in the `token_and_position_embedding` and the output `dense` layer!\n",
    "This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n",
    "while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Id31uCBUjswC",
    "outputId": "d9024ec6-8c36-495c-9ef9-063acaab902e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_6 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_7 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3387266 (12.92 MB)\n",
      "Trainable params: 3387266 (12.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdDfWypgjswC"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FrIpBzBjswC",
    "outputId": "c92ccefc-5bc8-4c9f-c376-ded41a714c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6337/6337 - 333s - loss: 4.4207 - perplexity: 83.4873 - val_loss: 4.1276 - val_perplexity: 62.5628 - 333s/epoch - 53ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 288s - loss: 4.0321 - perplexity: 56.5948 - val_loss: 4.0156 - val_perplexity: 55.9750 - 288s/epoch - 45ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 267s - loss: 3.9227 - perplexity: 50.7253 - val_loss: 3.9591 - val_perplexity: 52.7622 - 267s/epoch - 42ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 267s - loss: 3.8535 - perplexity: 47.3330 - val_loss: 3.8684 - val_perplexity: 48.3014 - 267s/epoch - 42ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 271s - loss: 3.8076 - perplexity: 45.2073 - val_loss: 3.8446 - val_perplexity: 47.0781 - 271s/epoch - 43ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 271s - loss: 3.7750 - perplexity: 43.7537 - val_loss: 3.8230 - val_perplexity: 46.1854 - 271s/epoch - 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7c8015bcf730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWJ3JE2Luky5"
   },
   "source": [
    "Model 2 performed slightly better than the original model, but not better than Model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vGHXvFFuJ3w"
   },
   "source": [
    "# Model 2 Hyperparameter Tuning - Identical to Model 2. In this run I did a grid search by trying out different numbers of layers and number of heads. The combinations I tried consisted of 4, 8, or 12 heads and 6 or 12 layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jew08GHL3dQa"
   },
   "source": [
    "## Build the model and Training\n",
    "Due to resource constraints not all possible combinations listed in the code below were run through, specifically 24 and 48 layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyFNrbzG3dQk",
    "outputId": "fc5502a7-c55c-40e2-84d3-615a2ec76804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers is 6 and Number of Heads is 4.\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_6 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_16 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_17 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_18 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_19 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_20 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_21 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4972424 (18.97 MB)\n",
      "Trainable params: 4972424 (18.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 606s - loss: 4.3432 - perplexity: 77.2578 - val_loss: 4.0801 - val_perplexity: 59.7420 - 606s/epoch - 96ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 507s - loss: 3.9133 - perplexity: 50.2502 - val_loss: 3.8586 - val_perplexity: 47.7656 - 507s/epoch - 80ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 507s - loss: 3.7788 - perplexity: 43.9251 - val_loss: 3.7464 - val_perplexity: 42.7369 - 507s/epoch - 80ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 502s - loss: 3.7019 - perplexity: 40.6705 - val_loss: 3.7222 - val_perplexity: 41.6399 - 502s/epoch - 79ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 503s - loss: 3.6513 - perplexity: 38.6632 - val_loss: 3.6874 - val_perplexity: 40.1952 - 503s/epoch - 79ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 504s - loss: 3.6142 - perplexity: 37.2529 - val_loss: 3.6530 - val_perplexity: 38.9726 - 504s/epoch - 80ms/step\n",
      "Number of Layers is 6 and Number of Heads is 8.\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_7 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_22 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_23 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_24 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_25 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_26 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_27 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4972424 (18.97 MB)\n",
      "Trainable params: 4972424 (18.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 603s - loss: 4.3377 - perplexity: 76.8352 - val_loss: 4.0162 - val_perplexity: 56.0462 - 603s/epoch - 95ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 538s - loss: 3.8872 - perplexity: 48.9570 - val_loss: 3.8406 - val_perplexity: 46.9219 - 538s/epoch - 85ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 538s - loss: 3.7515 - perplexity: 42.7383 - val_loss: 3.7508 - val_perplexity: 42.8855 - 538s/epoch - 85ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 537s - loss: 3.6743 - perplexity: 39.5612 - val_loss: 3.6794 - val_perplexity: 39.9227 - 537s/epoch - 85ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 539s - loss: 3.6232 - perplexity: 37.5907 - val_loss: 3.6718 - val_perplexity: 39.7469 - 539s/epoch - 85ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 556s - loss: 3.5865 - perplexity: 36.2333 - val_loss: 3.6354 - val_perplexity: 38.2640 - 556s/epoch - 88ms/step\n",
      "Number of Layers is 6 and Number of Heads is 12.\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_8 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_28 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_29 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_30 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_31 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_32 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_33 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4947776 (18.87 MB)\n",
      "Trainable params: 4947776 (18.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 695s - loss: 4.3332 - perplexity: 76.4976 - val_loss: 4.0044 - val_perplexity: 55.2961 - 695s/epoch - 110ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 607s - loss: 3.8773 - perplexity: 48.4740 - val_loss: 3.8045 - val_perplexity: 45.3673 - 607s/epoch - 96ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 617s - loss: 3.7452 - perplexity: 42.4731 - val_loss: 3.7508 - val_perplexity: 42.9100 - 617s/epoch - 97ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 600s - loss: 3.6691 - perplexity: 39.3575 - val_loss: 3.6602 - val_perplexity: 39.1811 - 600s/epoch - 95ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 607s - loss: 3.6183 - perplexity: 37.4075 - val_loss: 3.6680 - val_perplexity: 39.4992 - 607s/epoch - 96ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 608s - loss: 3.5810 - perplexity: 36.0376 - val_loss: 3.6404 - val_perplexity: 38.4823 - 608s/epoch - 96ms/step\n",
      "Number of Layers is 12 and Number of Heads is 4.\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_9 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_34 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_35 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_36 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_37 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_38 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_39 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_40 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_41 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_42 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_43 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_44 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_45 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7347080 (28.03 MB)\n",
      "Trainable params: 7347080 (28.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1022s - loss: 4.3151 - perplexity: 75.1182 - val_loss: 3.9861 - val_perplexity: 54.1270 - 1022s/epoch - 161ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 924s - loss: 3.8479 - perplexity: 47.0651 - val_loss: 3.7774 - val_perplexity: 43.8885 - 924s/epoch - 146ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 859s - loss: 3.7055 - perplexity: 40.8137 - val_loss: 3.6751 - val_perplexity: 39.7191 - 859s/epoch - 136ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 863s - loss: 3.6248 - perplexity: 37.6506 - val_loss: 3.6601 - val_perplexity: 39.0578 - 863s/epoch - 136ms/step\n",
      "Epoch 5/6\n"
     ]
    }
   ],
   "source": [
    "num_heads_values = [4, 8, 12]\n",
    "num_layers_values = [6, 12]\n",
    "\n",
    "for NUM_LAYERS_TEST in num_layers_values:\n",
    "  for NUM_HEADS_TEST in num_heads_values:\n",
    "    print(f\"Number of Layers is {NUM_LAYERS_TEST} and Number of Heads is {NUM_HEADS_TEST}.\")\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "    # Embedding.\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sequence_length=SEQ_LEN,\n",
    "        embedding_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    # Transformer decoders.\n",
    "    for _ in range(NUM_LAYERS_TEST):\n",
    "        decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "            num_heads=NUM_HEADS_TEST,\n",
    "            intermediate_dim=FEED_FORWARD_DIM,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            normalize_first=True,   #normalized similarly to gpt-2\n",
    "        )\n",
    "        x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "    # Output.\n",
    "    outputs_3 = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "    model_3 = keras.Model(inputs=inputs, outputs=outputs_3)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "    model_3.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n",
    "\n",
    "    model_3.summary()\n",
    "\n",
    "    model_3.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkHVFVPBBHYd",
    "outputId": "afe5c9fc-3b89-4934-d4aa-9fcf680acb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers is 12 and Number of Heads is 4.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_decoder (Trans  (None, None, 256)         395776    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_2 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_3 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_4 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_5 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_6 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_7 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_8 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_9 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_10 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_11 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7347080 (28.03 MB)\n",
      "Trainable params: 7347080 (28.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 952s - loss: 4.3266 - perplexity: 75.9891 - val_loss: 4.0040 - val_perplexity: 55.3149 - 952s/epoch - 150ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 860s - loss: 3.8548 - perplexity: 47.3950 - val_loss: 3.8100 - val_perplexity: 45.5289 - 860s/epoch - 136ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 857s - loss: 3.7112 - perplexity: 41.0482 - val_loss: 3.6989 - val_perplexity: 40.8102 - 857s/epoch - 135ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 859s - loss: 3.6261 - perplexity: 37.7001 - val_loss: 3.6777 - val_perplexity: 39.8907 - 859s/epoch - 135ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 856s - loss: 3.5724 - perplexity: 35.7246 - val_loss: 3.6060 - val_perplexity: 37.1591 - 856s/epoch - 135ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 853s - loss: 3.5296 - perplexity: 34.2279 - val_loss: 3.5967 - val_perplexity: 36.8275 - 853s/epoch - 135ms/step\n",
      "Number of Layers is 12 and Number of Heads is 8.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_1 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_12 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_13 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_14 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_15 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_16 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_17 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_18 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_19 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_20 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_21 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_22 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_23 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7347080 (28.03 MB)\n",
      "Trainable params: 7347080 (28.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 997s - loss: 4.3086 - perplexity: 74.6300 - val_loss: 3.9808 - val_perplexity: 53.8707 - 997s/epoch - 157ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 924s - loss: 3.8348 - perplexity: 46.4560 - val_loss: 3.7571 - val_perplexity: 43.1210 - 924s/epoch - 146ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 929s - loss: 3.6897 - perplexity: 40.1768 - val_loss: 3.6985 - val_perplexity: 40.7492 - 929s/epoch - 147ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 930s - loss: 3.6065 - perplexity: 36.9648 - val_loss: 3.6297 - val_perplexity: 38.0598 - 930s/epoch - 147ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 933s - loss: 3.5504 - perplexity: 34.9472 - val_loss: 3.5957 - val_perplexity: 36.7488 - 933s/epoch - 147ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 933s - loss: 3.5075 - perplexity: 33.4785 - val_loss: 3.5669 - val_perplexity: 35.7561 - 933s/epoch - 147ms/step\n",
      "Number of Layers is 12 and Number of Heads is 12.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_2 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_24 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_25 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_26 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_27 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_28 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_29 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_30 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_31 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_32 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_33 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_34 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_35 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7297784 (27.84 MB)\n",
      "Trainable params: 7297784 (27.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1077s - loss: 4.3055 - perplexity: 74.4052 - val_loss: 3.9708 - val_perplexity: 53.5004 - 1077s/epoch - 170ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 1001s - loss: 3.8238 - perplexity: 45.9499 - val_loss: 3.7802 - val_perplexity: 44.1847 - 1001s/epoch - 158ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 1021s - loss: 3.6810 - perplexity: 39.8293 - val_loss: 3.6727 - val_perplexity: 39.6853 - 1021s/epoch - 161ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 1019s - loss: 3.5978 - perplexity: 36.6470 - val_loss: 3.6264 - val_perplexity: 37.8776 - 1019s/epoch - 161ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 1017s - loss: 3.5421 - perplexity: 34.6615 - val_loss: 3.5827 - val_perplexity: 36.3250 - 1017s/epoch - 160ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 1026s - loss: 3.5003 - perplexity: 33.2388 - val_loss: 3.5562 - val_perplexity: 35.3452 - 1026s/epoch - 162ms/step\n",
      "Number of Layers is 24 and Number of Heads is 4.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_36 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_37 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_38 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_39 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_40 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_41 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_42 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_43 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_44 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_45 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_46 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_47 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_48 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_49 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_50 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_51 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_52 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_53 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_54 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_55 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_56 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_57 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_58 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_59 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12096392 (46.14 MB)\n",
      "Trainable params: 12096392 (46.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n"
     ]
    }
   ],
   "source": [
    "num_heads_values = [4, 8, 12]\n",
    "num_layers_values = [12, 24, 48]\n",
    "\n",
    "for NUM_LAYERS_TEST in num_layers_values:\n",
    "  for NUM_HEADS_TEST in num_heads_values:\n",
    "    print(f\"Number of Layers is {NUM_LAYERS_TEST} and Number of Heads is {NUM_HEADS_TEST}.\")\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "    # Embedding.\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sequence_length=SEQ_LEN,\n",
    "        embedding_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    # Transformer decoders.\n",
    "    for _ in range(NUM_LAYERS_TEST):\n",
    "        decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "            num_heads=NUM_HEADS_TEST,\n",
    "            intermediate_dim=FEED_FORWARD_DIM,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            normalize_first=True,   #normalized similarly to gpt-2\n",
    "        )\n",
    "        x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "    # Output.\n",
    "    outputs_3 = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "    model_3 = keras.Model(inputs=inputs, outputs=outputs_3)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "    model_3.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n",
    "\n",
    "    model_3.summary()\n",
    "\n",
    "    model_3.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeFS60fgO9fZ"
   },
   "source": [
    "# Model 3 - This model is identical to model 2 except for the addition of dropout layers with a dropout rate of 0.1. Specifically, a dropout layer was added after the embedding layer, before each of the transformer decoder layers, and right before the output layer.\n",
    "\n",
    "\n",
    "I added dropout layers to the model since dropout layers help prevent overfitting by randomly dropping neurons from the model's input and hidden  layers, thus helping the model to not overfocus on certain features.\n",
    "\n",
    "Sources:\n",
    "\n",
    "*   https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe#:~:text=Let's%20recap%20%E2%80%94%20dropout%20is%20a,the%20input%20and%20hidden%20layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-ZcfEj8Rz4Z"
   },
   "source": [
    "## Build the model and Training\n",
    "Due to resource constraints not all possible combinations listed in the code below were run through, specifically 24 and 48 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PJQrrTaO9fa",
    "outputId": "08eea750-3c97-49bf-b2c4-3259f8fc44e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers is 12 and Number of Heads is 4.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder (Trans  (None, None, 256)         395776    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_2 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_3 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_4 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_5 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_6 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_7 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_8 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_9 (Tra  (None, None, 256)         395776    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_10 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_11 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7347080 (28.03 MB)\n",
      "Trainable params: 7347080 (28.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 968s - loss: 4.7668 - perplexity: 118.0423 - val_loss: 4.3499 - val_perplexity: 78.2537 - 968s/epoch - 153ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 878s - loss: 4.3259 - perplexity: 75.9429 - val_loss: 4.1163 - val_perplexity: 61.9907 - 878s/epoch - 139ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 875s - loss: 4.1946 - perplexity: 66.5943 - val_loss: 4.0118 - val_perplexity: 55.8668 - 875s/epoch - 138ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 877s - loss: 4.1238 - perplexity: 62.0436 - val_loss: 3.9722 - val_perplexity: 53.6058 - 877s/epoch - 138ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 885s - loss: 4.0786 - perplexity: 59.2968 - val_loss: 3.9092 - val_perplexity: 50.4030 - 885s/epoch - 140ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 883s - loss: 4.0445 - perplexity: 57.3074 - val_loss: 3.9078 - val_perplexity: 50.2665 - 883s/epoch - 139ms/step\n",
      "Number of Layers is 12 and Number of Heads is 8.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_1 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_12 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_13 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_14 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_15 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_16 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_17 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_18 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_19 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_20 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_21 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_22 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_23 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7347080 (28.03 MB)\n",
      "Trainable params: 7347080 (28.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1024s - loss: 4.7602 - perplexity: 117.2679 - val_loss: 4.3125 - val_perplexity: 75.3646 - 1024s/epoch - 162ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 940s - loss: 4.3050 - perplexity: 74.3790 - val_loss: 4.1400 - val_perplexity: 63.4357 - 940s/epoch - 148ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 960s - loss: 4.1746 - perplexity: 65.2751 - val_loss: 4.0114 - val_perplexity: 55.6289 - 960s/epoch - 151ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 960s - loss: 4.1052 - perplexity: 60.8956 - val_loss: 3.9498 - val_perplexity: 52.4061 - 960s/epoch - 151ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 959s - loss: 4.0610 - perplexity: 58.2626 - val_loss: 3.9039 - val_perplexity: 49.9522 - 959s/epoch - 151ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 960s - loss: 4.0291 - perplexity: 56.4314 - val_loss: 3.9049 - val_perplexity: 49.8125 - 960s/epoch - 152ms/step\n",
      "Number of Layers is 12 and Number of Heads is 12.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_2 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_24 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_25 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_26 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_27 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_28 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_29 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_62 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_30 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_31 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_32 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_33 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_34 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_35 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7297784 (27.84 MB)\n",
      "Trainable params: 7297784 (27.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1111s - loss: 4.7518 - perplexity: 116.2807 - val_loss: 4.2632 - val_perplexity: 71.9236 - 1111s/epoch - 175ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 1007s - loss: 4.2948 - perplexity: 73.6186 - val_loss: 4.0496 - val_perplexity: 58.0014 - 1007s/epoch - 159ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 1006s - loss: 4.1644 - perplexity: 64.6148 - val_loss: 4.0139 - val_perplexity: 55.8538 - 1006s/epoch - 159ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 1004s - loss: 4.0936 - perplexity: 60.1944 - val_loss: 3.9274 - val_perplexity: 51.3034 - 1004s/epoch - 158ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 996s - loss: 4.0511 - perplexity: 57.6911 - val_loss: 3.9171 - val_perplexity: 50.7620 - 996s/epoch - 157ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 987s - loss: 4.0176 - perplexity: 55.7885 - val_loss: 3.9195 - val_perplexity: 50.8389 - 987s/epoch - 156ms/step\n",
      "Number of Layers is 24 and Number of Heads is 4.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_36 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_37 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_38 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_81 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_39 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_40 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_41 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_87 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_42 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_89 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_43 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_91 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_44 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_93 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_45 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_46 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_47 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_48 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_49 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_50 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_105 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_51 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_107 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_52 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_53 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_111 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_54 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_113 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_55 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_115 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_56 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_117 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_57 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_119 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_58 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_121 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " transformer_decoder_59 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dropout_123 (Dropout)       (None, None, 256)         0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12096392 (46.14 MB)\n",
      "Trainable params: 12096392 (46.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1663s - loss: 5.0744 - perplexity: 160.5079 - val_loss: 4.6210 - val_perplexity: 103.1001 - 1663s/epoch - 262ms/step\n",
      "Epoch 2/6\n"
     ]
    }
   ],
   "source": [
    "num_heads_values = [4, 8, 12]\n",
    "num_layers_values = [12, 24, 48]\n",
    "\n",
    "for NUM_LAYERS_TEST in num_layers_values:\n",
    "  for NUM_HEADS_TEST in num_heads_values:\n",
    "    print(f\"Number of Layers is {NUM_LAYERS_TEST} and Number of Heads is {NUM_HEADS_TEST}.\")\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "    # Embedding.\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sequence_length=SEQ_LEN,\n",
    "        embedding_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    # Transformer decoders.\n",
    "    for _ in range(NUM_LAYERS_TEST):\n",
    "        x = keras.layers.Dropout(rate=0.1)(x) # Added a dropout layer\n",
    "        decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "            num_heads=NUM_HEADS_TEST,\n",
    "            intermediate_dim=FEED_FORWARD_DIM,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            normalize_first=True,   #normalized similarly to gpt-2\n",
    "        )\n",
    "        x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "    # Output.\n",
    "    x = keras.layers.Dropout(rate=0.1)(x) # Added a dropout layer\n",
    "    outputs_4 = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "    model_4 = keras.Model(inputs=inputs, outputs=outputs_4)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "    model_4.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n",
    "\n",
    "    model_4.summary()\n",
    "\n",
    "    model_4.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_7IJZfYkzpN"
   },
   "source": [
    "# Model 4 - The fourth model I tested used reversible embedding instead of token and position embedding. The model included normalization layers similarly to model 2 and 3 as part of the transformer decoders. This model used the gelu_approximate activation instead of the default value of Relu as all the previous models used. Since dropout layers did not improve the metrics based on the results of the third model, I choose to not include them in this model.\n",
    "\n",
    "Sources: https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1QhFKHXn9ZC"
   },
   "source": [
    "## Build the model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0Ciw70Okzqb",
    "outputId": "57aaff02-0e33-41d7-cd5e-2d52a53ad608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers is 12 and Number of Heads is 4.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " reversible_embedding_2 (Re  (None, None, 256)         1280000   \n",
      " versibleEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_decoder_12 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_13 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_14 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_15 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_16 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_17 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_18 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_19 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_20 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_21 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_22 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_23 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7314312 (27.90 MB)\n",
      "Trainable params: 7314312 (27.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1013s - loss: 4.3275 - perplexity: 76.0307 - val_loss: 3.9528 - val_perplexity: 52.6611 - 1013s/epoch - 160ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 936s - loss: 3.8317 - perplexity: 46.3030 - val_loss: 3.7621 - val_perplexity: 43.3198 - 936s/epoch - 148ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 949s - loss: 3.6924 - perplexity: 40.2783 - val_loss: 3.7187 - val_perplexity: 41.6160 - 949s/epoch - 150ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 941s - loss: 3.6162 - perplexity: 37.3224 - val_loss: 3.6413 - val_perplexity: 38.4454 - 941s/epoch - 149ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 942s - loss: 3.5653 - perplexity: 35.4671 - val_loss: 3.6233 - val_perplexity: 37.8220 - 942s/epoch - 149ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 943s - loss: 3.5272 - perplexity: 34.1397 - val_loss: 3.6086 - val_perplexity: 37.2533 - 943s/epoch - 149ms/step\n",
      "Number of Layers is 12 and Number of Heads is 8.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " reversible_embedding_3 (Re  (None, None, 256)         1280000   \n",
      " versibleEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_decoder_24 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_25 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_26 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_27 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_28 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_29 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_30 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_31 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_32 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_33 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_34 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_35 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7314312 (27.90 MB)\n",
      "Trainable params: 7314312 (27.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1072s - loss: 4.2994 - perplexity: 73.9185 - val_loss: 3.9876 - val_perplexity: 54.5094 - 1072s/epoch - 169ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 995s - loss: 3.8203 - perplexity: 45.7793 - val_loss: 3.7735 - val_perplexity: 43.9098 - 995s/epoch - 157ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 1009s - loss: 3.6860 - perplexity: 40.0199 - val_loss: 3.7067 - val_perplexity: 41.0038 - 1009s/epoch - 159ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 1019s - loss: 3.6111 - perplexity: 37.1308 - val_loss: 3.6428 - val_perplexity: 38.3370 - 1019s/epoch - 161ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 1000s - loss: 3.5605 - perplexity: 35.2982 - val_loss: 3.6188 - val_perplexity: 37.5478 - 1000s/epoch - 158ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 1005s - loss: 3.5221 - perplexity: 33.9690 - val_loss: 3.5907 - val_perplexity: 36.5284 - 1005s/epoch - 159ms/step\n",
      "Number of Layers is 12 and Number of Heads is 12.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " reversible_embedding_4 (Re  (None, None, 256)         1280000   \n",
      " versibleEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_decoder_36 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_37 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_38 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_39 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_40 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_41 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_42 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_43 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_44 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_45 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_46 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_47 (Tr  (None, None, 256)         391668    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7265016 (27.71 MB)\n",
      "Trainable params: 7265016 (27.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1146s - loss: 4.2927 - perplexity: 73.4291 - val_loss: 3.9267 - val_perplexity: 51.0416 - 1146s/epoch - 181ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 1085s - loss: 3.8036 - perplexity: 45.0171 - val_loss: 3.7361 - val_perplexity: 42.1718 - 1085s/epoch - 171ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 1100s - loss: 3.6670 - perplexity: 39.2685 - val_loss: 3.6549 - val_perplexity: 38.8634 - 1100s/epoch - 174ms/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 1073s - loss: 3.5894 - perplexity: 36.3339 - val_loss: 3.6285 - val_perplexity: 37.9010 - 1073s/epoch - 169ms/step\n",
      "Epoch 5/6\n",
      "6337/6337 - 1077s - loss: 3.5375 - perplexity: 34.4952 - val_loss: 3.5643 - val_perplexity: 35.4813 - 1077s/epoch - 170ms/step\n",
      "Epoch 6/6\n",
      "6337/6337 - 1077s - loss: 3.5003 - perplexity: 33.2345 - val_loss: 3.5702 - val_perplexity: 35.8766 - 1077s/epoch - 170ms/step\n",
      "Number of Layers is 24 and Number of Heads is 4.\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " reversible_embedding_5 (Re  (None, None, 256)         1280000   \n",
      " versibleEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_decoder_48 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_49 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_50 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_51 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_52 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_53 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_54 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_55 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_56 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_57 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_58 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_59 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_60 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_61 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_62 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_63 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_64 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_65 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_66 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_67 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_68 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_69 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_70 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " transformer_decoder_71 (Tr  (None, None, 256)         395776    \n",
      " ansformerDecoder)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12063624 (46.02 MB)\n",
      "Trainable params: 12063624 (46.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "6337/6337 - 1821s - loss: 4.3230 - perplexity: 75.6833 - val_loss: 3.9623 - val_perplexity: 52.8702 - 1821s/epoch - 287ms/step\n",
      "Epoch 2/6\n"
     ]
    }
   ],
   "source": [
    "from keras_nlp.layers.modeling.reversible_embedding import ReversibleEmbedding\n",
    "from keras_nlp.utils.keras_utils import gelu_approximate\n",
    "num_heads_values = [4, 8, 12]\n",
    "num_layers_values = [12, 24, 48]\n",
    "\n",
    "for NUM_LAYERS_TEST in num_layers_values:\n",
    "  for NUM_HEADS_TEST in num_heads_values:\n",
    "    print(f\"Number of Layers is {NUM_LAYERS_TEST} and Number of Heads is {NUM_HEADS_TEST}.\")\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "    # Embedding.\n",
    "    embedding_layer = ReversibleEmbedding(\n",
    "        input_dim=VOCAB_SIZE,\n",
    "        #sequence_length=SEQ_LEN,\n",
    "        output_dim=EMBED_DIM, #256\n",
    "        mask_zero=True,\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    #x = keras_nlp.layers.PositionEmbedding(\n",
    "     #   sequence_length=SEQ_LEN,)(embedding_L)\n",
    "\n",
    "    # Transformer decoders.\n",
    "    for _ in range(NUM_LAYERS_TEST):\n",
    "        decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "            num_heads=NUM_HEADS_TEST,\n",
    "            intermediate_dim=FEED_FORWARD_DIM,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            activation=gelu_approximate, # uses gelu activation instead of default relu activation\n",
    "            normalize_first=True,   #normalized similarly to gpt-2\n",
    "        )\n",
    "        x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "    # Output.\n",
    "    outputs_5 = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "    model_5 = keras.Model(inputs=inputs, outputs=outputs_5)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "    model_5.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n",
    "\n",
    "    model_5.summary()\n",
    "\n",
    "    model_5.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ph1IS0mO-Zh"
   },
   "source": [
    "# Model 5 - Pre-Trained OPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6SxzPcJQQKl"
   },
   "source": [
    "## Load a pre-trained OPT model and generate some text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijtcySrOQQKl",
    "outputId": "d8d33bb2-a685-463f-a57e-6b77be51a91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/opt_125m_en/v1/vocab.json\n",
      "898822/898822 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/opt_125m_en/v1/merges.txt\n",
      "456318/456318 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/opt_125m_en/v1/model.h5\n",
      "501175368/501175368 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "preprocessor = keras_nlp.models.OPTCausalLMPreprocessor.from_preset(\n",
    "    \"opt_125m_en\",\n",
    "    sequence_length=SEQ_LEN,\n",
    ")\n",
    "opt_lm = keras_nlp.models.OPTCausalLM.from_preset(\n",
    "    \"opt_125m_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "z-Gk4_DmT4G2",
    "outputId": "7a4937b8-dc59-46b4-da5f-e094dc046ce0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"opt_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"opt_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ opt_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OPTTokenizer</span>)                       │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,265</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ opt_tokenizer (\u001b[38;5;33mOPTTokenizer\u001b[0m)                       │                                              \u001b[38;5;34m50,265\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"opt_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"opt_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ opt_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OPTBackbone</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">125,237,760</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50272</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,608,896</span> │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ opt_backbone (\u001b[38;5;33mOPTBackbone\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │     \u001b[38;5;34m125,237,760\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50272\u001b[0m)                    │      \u001b[38;5;34m38,608,896\u001b[0m │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,237,760</span> (477.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m125,237,760\u001b[0m (477.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,237,760</span> (477.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m125,237,760\u001b[0m (477.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhORxQkjQQKl"
   },
   "source": [
    "Once the model is loaded, you can use it to generate some text right away. Run\n",
    "the cells below to give it a try. It's as simple as calling a single function\n",
    "*generate()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQTQG0ZcQQKl",
    "outputId": "8e4c1ec1-a67b-4771-fe21-200a5d75e314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OPT output:\n",
      "My trip to Yosemite was a success. It was my first time to Yosemite and the first time I've ever been to the park. The park is great! I've only been there once before but it was a great experience.\n",
      "I was there once and I loved it. I had to get my passport back and was so happy to be there. It was a great experience.\n",
      "TOTAL TIME ELAPSED: 26.33s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "output = opt_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
    "print(\"\\nOPT output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JsiGq_tQQKl"
   },
   "source": [
    "Try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uddPGL29QQKl",
    "outputId": "7bc7df71-466b-4660-c741-5fe937da0e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OPT output:\n",
      "That Italian restaurant is so damn good and so cheap!\n",
      "The place is awesome.  The menu is pretty good.  I've been there before.\n",
      "I love the Italian food. The food in the restaurant was amazing, but the service was really bad. I'm not even a huge fan of Italian food, just the Italian ones. I've been in Italy and I've always loved Italian food, but I've never really been a fan of the food.\n",
      "You've never been a fan of the food?\n",
      "Nope. I've been a fan of the food since I was young, though I'm a sucker for Italian food (and Italian wine), and I'm a sucker for Italian food. I just never really liked the Italian food.\n",
      "TOTAL TIME ELAPSED: 1.81s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = opt_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "print(\"\\nOPT output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KocSV3fOPJre"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-gIZtc1PJrg"
   },
   "outputs": [],
   "source": [
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * EPOCHS,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B98USPPiio6p"
   },
   "outputs": [],
   "source": [
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "opt_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    "    loss=loss_fn,\n",
    "    metrics=[perplexity],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1TsNJadPJrh"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method.\n",
    "\n",
    "Due to resource constraints not all of the epochs ran through fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3psz5_E8PJrh",
    "outputId": "6fb4ba35-6781-4aa1-faa6-925fcd5b78fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6337/6337 - 6036s - loss: 3.7965 - perplexity: 75.5610 - accuracy: 0.2863 - val_loss: 3.3323 - val_perplexity: 88.0572 - val_accuracy: 0.2927 - 6036s/epoch - 953ms/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 5938s - loss: 3.7965 - perplexity: 75.5611 - accuracy: 0.2863 - val_loss: 3.3323 - val_perplexity: 88.0572 - val_accuracy: 0.2927 - 5938s/epoch - 937ms/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 5970s - loss: 3.7966 - perplexity: 75.5694 - accuracy: 0.2862 - val_loss: 3.3323 - val_perplexity: 88.0572 - val_accuracy: 0.2927 - 5970s/epoch - 942ms/step\n",
      "Epoch 4/6\n"
     ]
    }
   ],
   "source": [
    "#opt_lm.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)\n",
    "opt_lm.fit(raw_train_ds, validation_data=raw_val_ds, verbose=2, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgl74LZYnIzl"
   },
   "source": [
    "# Model 6 - Pre-Trained GPT2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEywB0xqnIzw"
   },
   "source": [
    "## Load a pre-trained GPT2 model and generate some text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF-SVbhHnIzw",
    "outputId": "68246e57-e4f4-4f34-9290-6760754f18ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
      "1042301/1042301 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
      "456318/456318 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
      "497986112/497986112 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=SEQ_LEN,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "_xf7SsX1nIzx",
    "outputId": "f53e26f8-6a3a-4ea5-f703-70613659bfb4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │     \u001b[38;5;34m124,439,808\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)                    │      \u001b[38;5;34m38,597,376\u001b[0m │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2F16paPnIzx"
   },
   "source": [
    "Once the model is loaded, you can use it to generate some text right away. Run\n",
    "the cells below to give it a try. It's as simple as calling a single function\n",
    "*generate()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHzDqyJ2nIzx",
    "outputId": "fb27ec0f-b622-496b-c9ca-3eb7b485b289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT2 output:\n",
      "My trip to Yosemite was a little different from what I expected. The first time I saw the Yosemite Valley is when I was a kid. This time I'm just going back to a time where I could see the entire valley. I didn't have any time to think about what was going on, and it was a little more relaxing and relaxing than before.\n",
      "\n",
      "I've been on a lot of hikes this past summer. I was able to get to Yosemite with friends and get a good view of the park and the Yosemite Valley.\n",
      "\n",
      "The first time I saw the Yosemite Valley, I was in a group of friends who were in the area for a hike and had some time to think. I was in the back seat of a car and was driving down the road. I didn't know what I was going to do and I thought, \"What is this place like?\" I was just driving around. I was just trying to think of what was going on. I was so\n",
      "TOTAL TIME ELAPSED: 32.56s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
    "print(\"\\nGPT2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK2BZF_OnIzx"
   },
   "source": [
    "Try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYwACIgInIzy",
    "outputId": "4bd0b8a7-bc90-4e4c-d53d-96fba7c55f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT2 output:\n",
      "That Italian restaurant is a good thing. The Italian-made pizza is delicious and the meat and vegetables are fresh and tender. The food has a nice touch and the service is very pleasant. The staff is friendly, attentive and attentive. I have been to the restaurant twice and I can say that this restaurant is one to visit. The food is very good and the staff are always friendly and knowledgeable. I have been coming here for about a month and am still waiting. The staff is very friendly and attentive and I am happy with my purchase. The food is good but I am not sure if it was the quality of the food or the service. I am a fan.\n",
      "TOTAL TIME ELAPSED: 1.69s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "print(\"\\nGPT2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fRLNSNohdch"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSY6q9t8nIzy"
   },
   "outputs": [],
   "source": [
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * EPOCHS,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmvWbqmYnIzy"
   },
   "outputs": [],
   "source": [
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    "    loss=loss_fn,\n",
    "    metrics=[perplexity],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzpsR_HenIzy"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmwNior_nIzy",
    "outputId": "83a9aa48-0064-4fce-98e5-f58ce1d24765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6337/6337 - 6792s - loss: 3.9490 - perplexity: 61.2845 - accuracy: 0.2697 - val_loss: 3.3554 - val_perplexity: 39.7276 - val_accuracy: 0.2976 - 6792s/epoch - 1s/step\n",
      "Epoch 2/6\n",
      "6337/6337 - 6710s - loss: 3.9487 - perplexity: 61.2680 - accuracy: 0.2697 - val_loss: 3.3554 - val_perplexity: 39.7276 - val_accuracy: 0.2976 - 6710s/epoch - 1s/step\n",
      "Epoch 3/6\n",
      "6337/6337 - 6715s - loss: 3.9486 - perplexity: 61.2597 - accuracy: 0.2697 - val_loss: 3.3554 - val_perplexity: 39.7276 - val_accuracy: 0.2976 - 6715s/epoch - 1s/step\n",
      "Epoch 4/6\n",
      "6337/6337 - 6719s - loss: 3.9487 - perplexity: 61.2712 - accuracy: 0.2697 - val_loss: 3.3554 - val_perplexity: 39.7276 - val_accuracy: 0.2976 - 6719s/epoch - 1s/step\n",
      "Epoch 5/6\n"
     ]
    }
   ],
   "source": [
    "gpt2_lm.fit(raw_train_ds, validation_data=raw_val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2jmDxlKjVQN"
   },
   "source": [
    "# Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmqKjIV4jhx4"
   },
   "source": [
    "The first model I tested was identical to the GPT Scratch Model from the \"GPT text generation from scratch with KerasNLP\" (original model) example except with the addition of a Normalization Layer. I added a normalization layer with the layer_norm_epsilon equal to 1e-5 to the original model since normalization layers have been shown to often help speed up and stabilize the learning process. A normalization layer helps prevent vanishing or exploding gradients which slows down learning. A normalization layer accomplishes this by keeping the weights and activations within a reasonable range. Additionally, I added this layer since it helps improve generalization of the model by reducing overfitting. Many of the top performing LLM's include it in their architecture. The loss, perplexity, validation loss, and validation perplexity all improved slightly relative to the original model, specifically the validation perplexity decreased to 44.5675 previously 47.4212 at epoch 6.\n",
    "\n",
    "The second model I tested is identical to Model 1 except in regards to the placement of the normalization layer. In Model 1 the normalization layer was added after the transformer decoders while in Model 2 it is added as part of the transformer decoders (keras_nlp.layers.TransformerDecoder), specifically I use the normalize_first argument and set it to true so the inputs to the attention layer and the intermediate dense layer are normalized in a fashion similar to a GPT-2 model. For both Model 1 and 2, I utilize 1e-5 as the eps value in the layer normalization component. Model 2 performed slightly better than the original model, but not better than Model 1.\n",
    "Next I continued to work with Model 2, specifically, I did a grid search by trying out different numbers of layers (6, 12) and number of heads (4, 8, 12) to optimize results. The original model used 3 heads and 2 layers. The model that performed best out of the ones I tried had 12 layers and 12 heads, it resulted in a validation perplexity of 35.34, which is lower than the 47.4212 at epoch 6 reported by the original model.\n",
    "\n",
    "The third model I tested is identical to model 2 except for the addition of dropout layers with a dropout rate of 0.1. Specifically, a dropout layer was added after the embedding layer, before each of the transformer decoder layers, and right before the output layer. I added dropout layers to the model since dropout layers help prevent overfitting by randomly dropping neurons from the model's input and hidden layers, thus helping the model to not overfocus on certain features. I also did a grid search by trying out different numbers of heads (4, 8, 12) with 12 layers to optimize results for this model. I didn’t try out 6 layers for this model since 12 layers were shown to perform better based on my results from model 2. The three variations in hyperparameter tuning for model 3 had the values for loss, perplexity, validation loss, and validation perplexity as worse off relative to the original model.\n",
    "\n",
    "The fourth model I tested used reversible embedding instead of token and position embedding. The model included normalization layers similarly to model 2 and 3 as part of the transformer decoders. This model used the gelu_approximate activation instead of the default value of Relu as all the previous models used. Since dropout layers did not improve the metrics based on the results of the third model, I choose to not include them in this model. I ran three variations of model 4 by varying the number of heads as 4, 8, or 12 with 12 layers. The optimal hyperparameter tuning for this model was 12 heads with 12 layers. Model 4 with 12 heads and 12 layers performed better than the original model, however, the metrics basically tied (were extremely close to) with model 2 that used 12 heads and 12 layers.\n",
    "\n",
    "The fifth model I tested used the pre-trained OPT model. The loss and validation loss had values similar to the original model, however surprisingly the perplexity and validation perplexity performed much worse relative to the original model. This model provided a validation perplexity of 88.05 while in the original model it was 47.4212.\n",
    "\n",
    "The sixth model I tested used the pre-trained GPT2 model. The loss and validation perplexity had a slightly higher value than the original model, thus it did not outperform the original model. The training perplexity had a much higher value relative to the validation perplexity.  \n",
    "\n",
    "\n",
    "Overall the model that performed the best was model 2 and 4 each using 12 layers and 12 heads.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
